{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292952d9",
   "metadata": {},
   "source": [
    "### Exercise 1 (3 points)\n",
    "\n",
    "1. Load the `sentence-transformers/multi-qa-mpnet-base-cos-v1` model and tokenizer. Use the `AutoModel` and\n",
    "   `AutoTokenizer` classes from `tranformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9b928e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacek-tyszkiewicz/lab_7MLProject/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3e0523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25ed82",
   "metadata": {},
   "source": [
    "2. Create a sample input text and tokenize it (padding, truncation, `return_tensors=\"pt\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313475a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"David and Emma looked at each other across the table. The young couple were happy: the food was delicious, the light from the candles was soft and the music was perfect. \"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14748d2e",
   "metadata": {},
   "source": [
    "3. Measure the inference time of the model in various inference modes (average time over 100 runs):\n",
    "   - no optimizations (simple PyTorch)\n",
    "   - `model.eval()`\n",
    "   - `model.eval()` and `no_grad()`\n",
    "   - `model.eval()` and `inference_mode()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0775cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "def measure_time(run_fn, n_runs: int = 100):\n",
    "    run_fn() #warm-up\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for _ in range(n_runs):\n",
    "        run_fn()\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    return (end - start) / n_runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581ff09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass (train mode): 0.005411 s\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "\n",
    "def forward_pass():\n",
    "    _= model(**inputs)\n",
    "\n",
    "time_train_mode = measure_time(forward_pass)\n",
    "print(f\"Forward pass (train mode): {time_train_mode:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06a3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass (eval mode): 0.004736 s\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "time_eval_mode = measure_time(forward_pass)\n",
    "print(f\"Forward pass (eval mode): {time_eval_mode:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882ede34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass (model.eval() + no_grad()): 0.003610 s\n"
     ]
    }
   ],
   "source": [
    "def forward_pass_no_grad():\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "\n",
    "time_eval_mode_no_grad = measure_time(forward_pass_no_grad)\n",
    "print(f\"Forward pass (model.eval() + no_grad()): {time_eval_mode_no_grad:.6f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144c08dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass (model.eval() + inference_mode()): 0.003282 s\n"
     ]
    }
   ],
   "source": [
    "def forward_pass_inference_mode():\n",
    "    with torch.inference_mode():\n",
    "        _ = model(**inputs)\n",
    "\n",
    "time_inf_eval_mode = measure_time(forward_pass_inference_mode)\n",
    "print(f\"Forward pass (model.eval() + inference_mode()): {time_inf_eval_mode:.6f} s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348eec67",
   "metadata": {},
   "source": [
    "4. Compare the speedup of options 2, 3, and 4 over the pure PyTorch. To calculate speedup, divide the\n",
    "   PyTorch time by the current time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f2de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup realtive to train mode:\n",
      "Forward pass: eval: 1.14x\n",
      "Forward pass: eval + no_grad: 1.50x\n",
      "Forward pass: eval + inference_mode: 1.65x\n"
     ]
    }
   ],
   "source": [
    "def speedup(base, other):\n",
    "    return base / other\n",
    "\n",
    "print(\"Speedup realtive to train mode:\")\n",
    "print(f\"Forward pass: eval: {speedup(time_train_mode, time_eval_mode):.2f}x\")\n",
    "print(f\"Forward pass: eval + no_grad: {speedup(time_train_mode, time_eval_mode_no_grad):.2f}x\")\n",
    "print(f\"Forward pass: eval + inference_mode: {speedup(time_train_mode, time_inf_eval_mode):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f0d15",
   "metadata": {},
   "source": [
    "### Exercise 2 (2 points)\n",
    "\n",
    "In this exercise, we will verify the gains from model compilation with `torch.compile()`.\n",
    "\n",
    "1. Compile the model using `torch.compile()` after switching it to evaluation mode, and warm-up the model\n",
    "   by running a single inference call. Measure this compilation + warm-up time (just once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d21fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation + warm-up time: 8.008003\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "model.eval()\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "def forward_pass_inference_mode_model_compile():\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_model(**inputs)\n",
    "\n",
    "forward_pass_inference_mode_model_compile()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Compilation + warm-up time: {(end - start):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3aff80",
   "metadata": {},
   "source": [
    "2. Measure the inference time (average of 100 runs) of the compiled model in inference mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4103cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass (eval + inference_mode+ compile): 0.002651 s\n"
     ]
    }
   ],
   "source": [
    "time_inf_eval_mode_compile = measure_time(forward_pass_inference_mode_model_compile)\n",
    "print(f\"Forward pass (eval + inference_mode+ compile): {time_inf_eval_mode_compile:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5561e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass: eval + inference_mode + comile: 2.04x\n"
     ]
    }
   ],
   "source": [
    "print(f\"Forward pass: eval + inference_mode + comile: {speedup(time_train_mode, time_inf_eval_mode_compile):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a74e4",
   "metadata": {},
   "source": [
    "The compiled model is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d520675",
   "metadata": {},
   "source": [
    "### Exercise 3 (3 points)\n",
    "\n",
    "We will perform a dynamic quantization for our model, which is very simple operationally to use with PyTorch.\n",
    "It provides the `torch.ao.quantization.quantize_dynamic()` function, to which we pass the model and a \n",
    "list of layer types that we want to quantize. In the case of transformers, those are primarily the linear\n",
    "layers, which contain the majority of weights and perform most computations.\n",
    "\n",
    "1. Ensure the model is on the CPU.\n",
    "2. Quantize the model with `torch.ao.quantization.quantize_dynamic()`, setting the target weight to `torch.qint8` and\n",
    "   layers to a single-element set with `nn.Linear`.\n",
    "3. Save the model to a new variable (e.g. `model_quantized`), and print it to verify that linear layers have been\n",
    "   quantized properly (i.e. `DynamicQuantizedLinear` instead of `Linear`).\n",
    "4. Save both models to disk (`state_dict` for both) and compare the file sizes (e.g. `os.path.getsize()`).\n",
    "5. Compare the inference speed and speedup on CPU for original and quantized models (again, average of 100 runs).\n",
    "6. Display the comparison. Do you think that quantization is helpful in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc5d2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.ao.quantization import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54fd7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model\n",
      "MPNetModel(\n",
      "  (embeddings): MPNetEmbeddings(\n",
      "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): MPNetEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x MPNetLayer(\n",
      "        (attention): MPNetAttention(\n",
      "          (attn): MPNetSelfAttention(\n",
      "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (intermediate): MPNetIntermediate(\n",
      "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): MPNetOutput(\n",
      "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (relative_attention_bias): Embedding(32, 12)\n",
      "  )\n",
      "  (pooler): MPNetPooler(\n",
      "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "File size\n",
      "fp32 model size: 417.73 MB\n",
      "int8 model size: 173.10 MB\n",
      "size comparsion (int8 / fp32): 0.41\n"
     ]
    }
   ],
   "source": [
    "device_cpu = torch.device(\"cpu\")\n",
    "\n",
    "model_cpu = model.to(device_cpu)\n",
    "model_cpu.eval()\n",
    "\n",
    "inputs_cpu = {k: v.to(device_cpu) for k, v in inputs.items()}\n",
    "\n",
    "model_quantized = quantize_dynamic(\n",
    "    model_cpu,             \n",
    "    {nn.Linear},           \n",
    "    dtype=torch.qint8     \n",
    ")\n",
    "\n",
    "print(\"Quantized model\")\n",
    "print(model_quantized)\n",
    "\n",
    "\n",
    "fp32_path = \"model_fp32_state_dict.pth\"\n",
    "int8_path = \"model_quantized_int8_state_dict.pth\"\n",
    "\n",
    "torch.save(model_cpu.state_dict(), fp32_path)\n",
    "torch.save(model_quantized.state_dict(), int8_path)\n",
    "\n",
    "size_fp32 = os.path.getsize(fp32_path)\n",
    "size_int8 = os.path.getsize(int8_path)\n",
    "\n",
    "def to_MB(bytes_size):\n",
    "    return bytes_size / (1024 * 1024)\n",
    "\n",
    "print(\"File size\")\n",
    "print(f\"fp32 model size: {to_MB(size_fp32):.2f} MB\")\n",
    "print(f\"int8 model size: {to_MB(size_int8):.2f} MB\")\n",
    "print(f\"size comparsion (int8 / fp32): {size_int8 / size_fp32:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa76b3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference comparsion fp32 vs int8 on CPU\n",
      "fp32 model: 0.027787 s\n",
      "int8 model: 0.011173 s\n",
      "Speedup int8 vs fp32: 2.49x\n"
     ]
    }
   ],
   "source": [
    "def forward_fp32():\n",
    "    with torch.inference_mode():\n",
    "        _ = model_cpu(**inputs_cpu)\n",
    "\n",
    "\n",
    "def forward_int8():\n",
    "    with torch.inference_mode():\n",
    "        _ = model_quantized(**inputs_cpu)\n",
    "\n",
    "\n",
    "time_fp32 = measure_time(forward_fp32, n_runs=100)\n",
    "time_int8 = measure_time(forward_int8, n_runs=100)\n",
    "\n",
    "print(\"Inference comparsion fp32 vs int8 on CPU\")\n",
    "print(f\"fp32 model: {time_fp32:.6f} s\")\n",
    "print(f\"int8 model: {time_int8:.6f} s\")\n",
    "\n",
    "speedup_int8 = time_fp32 / time_int8\n",
    "print(f\"Speedup int8 vs fp32: {speedup_int8:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0a873",
   "metadata": {},
   "source": [
    "Quantization is helpful here. The int8 quantized model is faster and smaller than the model that uses fp32 precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a527c",
   "metadata": {},
   "source": [
    "### Exercise 4 (2 points)\n",
    "\n",
    "1. Compare inference time of:\n",
    "   - `torch.compile()` with default settings\n",
    "   - `torch.compile()` with `mode=\"max-autotune\"`\n",
    "   - `torch.compile()` with `mode=\"max-autotune-no-cudagraphs\"`\n",
    "2. Report the average time of 100 runs and speedup of the latter two modes.\n",
    "\n",
    "Check a few different text input sizes. What happens in the latter two modes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad5c0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model = model.to(device)\n",
    "#inputs_gpu = {k: v.to(device) for k, v in inputs.items()}\n",
    "#with torch.inference_mode():\n",
    "#    outputs = model(**inputs_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b573a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiled_model_with_cudagraphs = torch.compile(model, mode=\"max-autotune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiled_model_dynamic = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69980274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\", pin_memory=True)\n",
    "\n",
    "#from torch.utils.data import DataLoader\n",
    "\n",
    "#dataloader = DataLoader(dataset, batch_size=32, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa97dfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "base_model = model.to(device)\n",
    "base_model.eval()\n",
    "\n",
    "texts = {\n",
    "    \"short\": \"Kate looked around the room at the other people: ten men and ten women all around the same age. \",\n",
    "    \"medium\": \"Sergeant Frank Spike sat behind his desk and looked out of the window. \"\n",
    "              \"Outside, cars moved slowly in the cold, grey rain. He looked down at the grey hairs on his arms.\",\n",
    "    \"long\": \"Dr Tomas Streyer looked around the control room at his team of scientists and engineers. \"\n",
    "            \"He was excited and frightened but he tried to seem calm. In a few minutes, they might start to \"\n",
    "            \"discover something amazing: how the universe began. He looked out of the window at the beautiful\"\n",
    "            \" blue summer sky and tried to breathe slowly. 'Ready,' he said. He pressed the first button and the \"\n",
    "            \"complicated computers and machines came to life. 'Set,' he said. He pressed the second button and \"\n",
    "            \"switched on the large particle accelerator that lay under the towns and fields of Switzerland. \",\n",
    "}\n",
    "\n",
    "def make_inputs(text: str):\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "\n",
    "model_default = torch.compile(base_model)\n",
    "model_max_auto = torch.compile(copy.deepcopy(base_model), mode=\"max-autotune\")\n",
    "model_max_auto_nocuda = torch.compile(copy.deepcopy(base_model),mode=\"max-autotune-no-cudagraphs\")\n",
    "\n",
    "def make_forward_fn(compiled_model, inputs_gpu):\n",
    "    def forward():\n",
    "        with torch.inference_mode():\n",
    "            _ = compiled_model(**inputs_gpu)\n",
    "    return forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "126950f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparsion different torch.compile() modes - inference on GPU\n",
      "Input size: short\n",
      "\n",
      "default compile: 0.002623s\n",
      "\n",
      "max-autotune: 0.002324s\n",
      " (speedup vs default: 1.13x)\n",
      "\n",
      "max-autotune-no-cudagraphs: 0.002355s\n",
      " (speedup vs default: 1.11x)\n",
      "\n",
      "Input size: medium\n",
      "\n",
      "default compile: 0.002575s\n",
      "\n",
      "max-autotune: 0.002442s\n",
      " (speedup vs default: 1.05x)\n",
      "\n",
      "max-autotune-no-cudagraphs: 0.002560s\n",
      " (speedup vs default: 1.01x)\n",
      "\n",
      "Input size: long\n",
      "\n",
      "default compile: 0.004015s\n",
      "\n",
      "max-autotune: 0.003885s\n",
      " (speedup vs default: 1.03x)\n",
      "\n",
      "max-autotune-no-cudagraphs: 0.003988s\n",
      " (speedup vs default: 1.01x)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparsion different torch.compile() modes - inference on GPU\")\n",
    "for name, text in texts.items():\n",
    "    print(f\"Input size: {name}\\n\")\n",
    "\n",
    "    inputs_gpu = make_inputs(text)\n",
    "\n",
    "    #warm-up\n",
    "    with torch.inference_mode():\n",
    "        _ = model_default(**inputs_gpu)\n",
    "        _ = model_max_auto(**inputs_gpu)\n",
    "        _ = model_max_auto_nocuda(**inputs_gpu)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t_default = measure_time(make_forward_fn(model_default, inputs_gpu))\n",
    "    t_max_auto = measure_time(make_forward_fn(model_max_auto, inputs_gpu))\n",
    "    t_max_auto_nocg = measure_time(make_forward_fn(model_max_auto_nocuda, inputs_gpu))\n",
    "\n",
    "    print(f\"default compile: {t_default:.6f}s\\n\")\n",
    "\n",
    "    print(f\"max-autotune: {t_max_auto:.6f}s\\n \"\n",
    "          f\"(speedup vs default: {t_default / t_max_auto:.2f}x)\\n\")\n",
    "    print(f\"max-autotune-no-cudagraphs: {t_max_auto_nocg:.6f}s\\n \"\n",
    "          f\"(speedup vs default: {t_default / t_max_auto_nocg:.2f}x)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267f5cb",
   "metadata": {},
   "source": [
    "For this model the three torch.compile() modes are very close.  \n",
    "max-autotune is only a little faster than the default (about 3â€“13 p.p).  \n",
    "max-autotune-no-cudagraphs is almost the same as the default.  \n",
    "Changing the input length (short / medium / long) does not   \n",
    "change the results much.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a228c",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 5 (2 points)\n",
    "\n",
    "1. Check if your GPU supports Tensor Cores (capability >= (7,0)). If not, switch to Google Colab with GPU runtime.\n",
    "2. Measure inference time with:\n",
    "   - full precision (`float32`)\n",
    "   - manual half-precision (`float16`)\n",
    "   - automatic mixed precision (`torch.autocast`)\n",
    "3. Compare time and speedup. Which variant would you use in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c73aacb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device capability: (8, 9)\n",
      "Tensor Cores available: fast float16 supported.\n"
     ]
    }
   ],
   "source": [
    "capability = torch.cuda.get_device_capability()\n",
    "print(f\"CUDA device capability: {capability}\")\n",
    "\n",
    "if capability >= (7, 0):\n",
    "    print(\"Tensor Cores available: fast float16 supported.\")\n",
    "else:\n",
    "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "110ca3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"].to(\"cuda\")        \n",
    "attention_mask = inputs[\"attention_mask\"].to(\"cuda\") \n",
    "\n",
    "model_half = model.half().to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model_half(input_ids=input_ids,attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79f4178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(model_half.dtype)                   \n",
    "print(outputs.last_hidden_state.dtype)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25e8f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of model_fp32 parameters: torch.float32\n",
      "Data type of data_fp32: torch.float32\n",
      "Data type of labels_fp32: torch.float32\n",
      "Loss fp32: 1.1362165212631226\n",
      "Data type of model_fp16 parameters: torch.float16\n",
      "Data type of data_fp16: torch.float16\n",
      "Data type of labels_fp16: torch.float16\n",
      "Loss fp16: 1.136107325553894\n"
     ]
    }
   ],
   "source": [
    "model_fp32 = torch.nn.Linear(10, 1)\n",
    "data_fp32 = torch.randn(100, 10)\n",
    "labels_fp32 = torch.randn(100, 1)\n",
    "\n",
    "print(f\"Data type of model_fp32 parameters: {model_fp32.weight.dtype}\")\n",
    "print(f\"Data type of data_fp32: {data_fp32.dtype}\")\n",
    "print(f\"Data type of labels_fp32: {labels_fp32.dtype}\")\n",
    "\n",
    "output_fp32 = model_fp32(data_fp32)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "loss_fp32 = loss_fn(output_fp32, labels_fp32)\n",
    "\n",
    "print(f\"Loss fp32: {loss_fp32.item()}\")\n",
    "\n",
    "model_fp16 = model_fp32.half()\n",
    "data_fp16 = data_fp32.half()\n",
    "labels_fp16 = labels_fp32.half()\n",
    "\n",
    "print(f\"Data type of model_fp16 parameters: {model_fp16.weight.dtype}\")\n",
    "print(f\"Data type of data_fp16: {data_fp16.dtype}\")\n",
    "print(f\"Data type of labels_fp16: {labels_fp16.dtype}\")\n",
    "\n",
    "output_fp16 = model_fp16(data_fp16)\n",
    "loss_fp16 = loss_fn(output_fp16.float(), labels_fp16.float())\n",
    "\n",
    "print(f\"Loss fp16: {loss_fp16.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63797106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of model_fp16 parameters: torch.float16\n",
      "Data type of data_fp16: torch.float16\n",
      "Data type of labels_fp16: torch.float16\n",
      "Loss fp16: 1.136107325553894\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = model_fp32.half()\n",
    "data_fp16 = data_fp32.half()\n",
    "labels_fp16 = labels_fp32.half()\n",
    "\n",
    "print(f\"Data type of model_fp16 parameters: {model_fp16.weight.dtype}\")\n",
    "print(f\"Data type of data_fp16: {data_fp16.dtype}\")\n",
    "print(f\"Data type of labels_fp16: {labels_fp16.dtype}\")\n",
    "\n",
    "output_fp16 = model_fp16(data_fp16)\n",
    "loss_fp16 = loss_fn(output_fp16.float(), labels_fp16.float())\n",
    "\n",
    "print(f\"Loss fp16: {loss_fp16.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e8510",
   "metadata": {},
   "source": [
    "2. Measure inference time with:\n",
    "   - full precision (`float32`)\n",
    "   - manual half-precision (`float16`)\n",
    "   - automatic mixed precision (`torch.autocast`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dec638f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparison of numerical precisions \n",
      "\n",
      "input size: short\n",
      "float32:0.003158s\n",
      "float16 (manual):0.003022s (speedup vs fp32: 1.04x)\n",
      "autocast (AMP):0.003655s speedup vs fp32: 0.86x)\n",
      "\n",
      "input size: medium\n",
      "float32:0.002786s\n",
      "float16 (manual):0.002765s (speedup vs fp32: 1.01x)\n",
      "autocast (AMP):0.003556s speedup vs fp32: 0.78x)\n",
      "\n",
      "input size: long\n",
      "float32:0.002974s\n",
      "float16 (manual):0.002874s (speedup vs fp32: 1.03x)\n",
      "autocast (AMP):0.003794s speedup vs fp32: 0.78x)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_fp32 = base_model\n",
    "\n",
    "model_fp16 = copy.deepcopy(base_model).half().to(device)\n",
    "\n",
    "\n",
    "def forward_fp32(inputs):\n",
    "    with torch.inference_mode():\n",
    "        _ = model_fp32(**inputs)\n",
    "\n",
    "def forward_fp16(inputs):\n",
    "    with torch.inference_mode():\n",
    "        _ = model_fp16(**inputs)\n",
    "\n",
    "def forward_autocast(inputs):\n",
    "    with torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        _ = model_fp32(**inputs)\n",
    "\n",
    "\n",
    "print(\"comparison of numerical precisions \\n\")\n",
    "\n",
    "for name, text in texts.items():\n",
    "    print(f\"input size: {name}\")\n",
    "\n",
    "    inputs = make_inputs(text)\n",
    "\n",
    "    t_fp32 = measure_time(lambda: forward_fp32(inputs))\n",
    "    t_fp16 = measure_time(lambda: forward_fp16(inputs))\n",
    "    t_amp  = measure_time(lambda: forward_autocast(inputs))\n",
    "\n",
    "    print(f\"float32:{t_fp32:.6f}s\")\n",
    "    print(f\"float16 (manual):{t_fp16:.6f}s (speedup vs fp32: {t_fp32 / t_fp16:.2f}x)\")\n",
    "    print(f\"autocast (AMP):{t_amp:.6f}s speedup vs fp32: {t_fp32 / t_amp:.2f}x)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813a8d5",
   "metadata": {},
   "source": [
    "In my model the time differences are small. Manual float16 is only a little faster than float32 (about 1.01 and 1.04).  \n",
    "The autocast mode os slower, because it has extra overhead. In practice, for such a small model, I would choose normal  \n",
    "float32 or manual float16  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc52456",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 6 (3 points)\n",
    "\n",
    "1. Measure cold start time (including session creation) of the ONNX model using online and offline optimization modes\n",
    "   on CPU.\n",
    "2. Measure inference time of the ONNX model on CPU using both optimization modes.\n",
    "3. Prepare deployment Docker images:\n",
    "   - build two images, for a) compiled PyTorch model b) ONNX model with ONNX Runtime\n",
    "   - select the best model in both cases in terms of the inference time\n",
    "   - install a minimal set of requirements in both cases, e.g. do not install PyTorch for ONNX image\n",
    "4. Compare for those apps:\n",
    "   - Docker container sizes\n",
    "   - response time (average of 100 requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "model_cpu = model.eval().cpu()\n",
    "\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX export.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "    \"model.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacafca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "ort_session = ort.InferenceSession(\"model.onnx\")\n",
    "\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX inference.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "\n",
    "\n",
    "inputs_onnx = {\n",
    "    \"input_ids\": sample_input[\"input_ids\"],\n",
    "    \"attention_mask\": sample_input[\"attention_mask\"],\n",
    "}\n",
    "\n",
    "outputs_onnx = ort_session.run(None, inputs_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b3aca0",
   "metadata": {},
   "source": [
    "### Online mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9159e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold start (online): 0.472560s\n"
     ]
    }
   ],
   "source": [
    "options = ort.SessionOptions()\n",
    "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "start = time.perf_counter()\n",
    "session_online = ort.InferenceSession(\n",
    "    \"model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "_ = session_online.run(None, inputs_onnx) \n",
    "end = time.perf_counter()\n",
    "\n",
    "cold_online = end - start\n",
    "print(f\"cold start (online): {cold_online:.6f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c619e",
   "metadata": {},
   "source": [
    "### Inference time online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8e2c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time (online): 0.013596s\n"
     ]
    }
   ],
   "source": [
    "t_online = measure_time(lambda: session_online.run(None, inputs_onnx), n_runs=100)\n",
    "print(f\"Inference time (online): {t_online:.6f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f922d4b",
   "metadata": {},
   "source": [
    "### Offline mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242975fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold start (offline build): 0.577415s\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "sess_options.optimized_model_filepath = \"model_optimized.onnx\"\n",
    "\n",
    "start = time.perf_counter()\n",
    "session_build = ort.InferenceSession(\"model.onnx\", sess_options)\n",
    "_ = session_build.run(None, inputs_onnx)  \n",
    "end = time.perf_counter()\n",
    "\n",
    "cold_offline = end - start\n",
    "print(f\"cold start (offline build): {cold_offline:.6f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time (offline, optimized model): 0.026927s\n",
      "speedup offline vs online: 0.50x\n"
     ]
    }
   ],
   "source": [
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "\n",
    "ort_session_optimized = ort.InferenceSession(\n",
    "    \"model_optimized.onnx\", \n",
    "    sess_options=sess_options, \n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "\n",
    "\n",
    "t_offline = measure_time(\n",
    "    lambda: ort_session_optimized.run(None, inputs_onnx), \n",
    "    n_runs=100\n",
    ")\n",
    "print(f\"inference time (offline, optimized model): {t_offline:.6f}s\")\n",
    "\n",
    "print(f\"speedup offline vs online: {t_online / t_offline:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e3d24",
   "metadata": {},
   "source": [
    "4. Compare for those apps:\n",
    "   - Docker container sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3a4ad",
   "metadata": {},
   "source": [
    "Docker container sizes\n",
    "\n",
    "- PyTorch image (`lab7-torch:latest`): = 7.87 GB\n",
    "- ONNX image (`lab7-onnx:latest`): = 726 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f329381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring 100 requests for http://localhost:8000/predict\n",
      "avg: 20.91 ms\n",
      "min: 17.92 ms\n",
      "max: 27.04 ms\n",
      "\n",
      "Measuring 100 requests for http://localhost:8001/predict\n",
      "avg: 15.08 ms\n",
      "min: 12.67 ms\n",
      "max: 18.58 ms\n",
      "\n",
      "Speedup ONNX vs PyTorch: 1.39x\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics as stats\n",
    "import requests\n",
    "\n",
    "payload = {\"text\": \"Dr Tomas Streyer looked around the control room at his team of scientists and engineers.\"}\n",
    "N_RUNS = 100\n",
    "\n",
    "def measure_endpoint(url: str, n_runs: int = N_RUNS):\n",
    "    print(f\"Measuring {n_runs} requests for {url}\")\n",
    "\n",
    "    # warm-up \n",
    "    for _ in range(5):\n",
    "        requests.post(url, json=payload)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        t0 = time.perf_counter()\n",
    "        r = requests.post(url, json=payload)\n",
    "        r.raise_for_status()\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "\n",
    "    avg = stats.mean(times)\n",
    "    print(f\"avg: {avg*1000:.2f} ms\")\n",
    "    print(f\"min: {min(times)*1000:.2f} ms\")\n",
    "    print(f\"max: {max(times)*1000:.2f} ms\")\n",
    "    print()\n",
    "    return avg\n",
    "\n",
    "torch_avg = measure_endpoint(\"http://localhost:8000/predict\")\n",
    "onnx_avg  = measure_endpoint(\"http://localhost:8001/predict\")\n",
    "print(f\"Speedup ONNX vs PyTorch: {torch_avg / onnx_avg:.2f}x\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-course-agh-lab-07 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
