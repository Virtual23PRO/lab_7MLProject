{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292952d9",
   "metadata": {},
   "source": [
    "### Exercise 1 (3 points)\n",
    "\n",
    "1. Load the `sentence-transformers/multi-qa-mpnet-base-cos-v1` model and tokenizer. Use the `AutoModel` and\n",
    "   `AutoTokenizer` classes from `tranformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9b928e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacek-tyszkiewicz/lab_7MLProject/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3e0523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25ed82",
   "metadata": {},
   "source": [
    "2. Create a sample input text and tokenize it (padding, truncation, `return_tensors=\"pt\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313475a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"David and Emma looked at each other across the table. The young couple were happy: the food was delicious, the light from the candles was soft and the music was perfect. \"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14748d2e",
   "metadata": {},
   "source": [
    "3. Measure the inference time of the model in various inference modes (average time over 100 runs):\n",
    "   - no optimizations (simple PyTorch)\n",
    "   - `model.eval()`\n",
    "   - `model.eval()` and `no_grad()`\n",
    "   - `model.eval()` and `inference_mode()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0775cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "def measure_time(run_fn, n_runs: int = 100):\n",
    "    run_fn() #warm-up, don't measure time\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for _ in range(n_runs):\n",
    "        run_fn()\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    return (end - start) / n_runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "581ff09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass (train mode): 0.005723 s\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "\n",
    "def forward_pass():\n",
    "    _= model(**inputs)\n",
    "\n",
    "time_train_mode = measure_time(forward_pass)\n",
    "print(f\"Forward pass (train mode): {time_train_mode:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b06a3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass (eval mode): 0.004994 s\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "time_eval_mode = measure_time(forward_pass)\n",
    "print(f\"Forward pass (eval mode): {time_eval_mode:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "882ede34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass (model.eval() + no_grad()): 0.003649 s\n"
     ]
    }
   ],
   "source": [
    "def forward_pass_no_grad():\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "\n",
    "time_eval_mode_no_grad = measure_time(forward_pass_no_grad)\n",
    "print(f\"Forward pass (model.eval() + no_grad()): {time_eval_mode_no_grad:.6f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "144c08dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass (model.eval() + inference_mode()): 0.003471 s\n"
     ]
    }
   ],
   "source": [
    "def forward_pass_inference_mode():\n",
    "    with torch.inference_mode():\n",
    "        _ = model(**inputs)\n",
    "\n",
    "time_inf_eval_mode = measure_time(forward_pass_inference_mode)\n",
    "print(f\"Forward pass (model.eval() + inference_mode()): {time_inf_eval_mode:.6f} s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348eec67",
   "metadata": {},
   "source": [
    "4. Compare the speedup of options 2, 3, and 4 over the pure PyTorch. To calculate speedup, divide the\n",
    "   PyTorch time by the current time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86f2de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup realtive to train mode:\n",
      "Forward pass: eval: 1.15x\n",
      "Forward pass: eval + no_grad: 1.57x\n",
      "Forward pass: eval + inference_mode: 1.65x\n"
     ]
    }
   ],
   "source": [
    "def speedup(base, other):\n",
    "    return base / other\n",
    "\n",
    "print(\"Speedup realtive to train mode:\")\n",
    "print(f\"Forward pass: eval: {speedup(time_train_mode, time_eval_mode):.2f}x\")\n",
    "print(f\"Forward pass: eval + no_grad: {speedup(time_train_mode, time_eval_mode_no_grad):.2f}x\")\n",
    "print(f\"Forward pass: eval + inference_mode: {speedup(time_train_mode, time_inf_eval_mode):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f0d15",
   "metadata": {},
   "source": [
    "### Exercise 2 (2 points)\n",
    "\n",
    "In this exercise, we will verify the gains from model compilation with `torch.compile()`.\n",
    "\n",
    "1. Compile the model using `torch.compile()` after switching it to evaluation mode, and warm-up the model\n",
    "   by running a single inference call. Measure this compilation + warm-up time (just once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77d21fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation + warm-up time: 0.019686\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "model.eval()\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "def forward_pass_inference_mode_model_compile():\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_model(**inputs)\n",
    "\n",
    "forward_pass_inference_mode_model_compile()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Compilation + warm-up time: {(end - start):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3aff80",
   "metadata": {},
   "source": [
    "2. Measure the inference time (average of 100 runs) of the compiled model in inference mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4103cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass (eval + inference_mode+ compile): 0.003350 s\n"
     ]
    }
   ],
   "source": [
    "time_inf_eval_mode_compile = measure_time(forward_pass_inference_mode_model_compile)\n",
    "print(f\"Forward pass (eval + inference_mode+ compile): {time_inf_eval_mode_compile:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5561e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass: eval + inference_mode + comile: 1.71x\n"
     ]
    }
   ],
   "source": [
    "print(f\"Forward pass: eval + inference_mode + comile: {speedup(time_train_mode, time_inf_eval_mode_compile):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a74e4",
   "metadata": {},
   "source": [
    "The compiled model is only a little faster.\n",
    "In many runs the time is almost the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d520675",
   "metadata": {},
   "source": [
    "### Exercise 3 (3 points)\n",
    "\n",
    "We will perform a dynamic quantization for our model, which is very simple operationally to use with PyTorch.\n",
    "It provides the `torch.ao.quantization.quantize_dynamic()` function, to which we pass the model and a \n",
    "list of layer types that we want to quantize. In the case of transformers, those are primarily the linear\n",
    "layers, which contain the majority of weights and perform most computations.\n",
    "\n",
    "1. Ensure the model is on the CPU.\n",
    "2. Quantize the model with `torch.ao.quantization.quantize_dynamic()`, setting the target weight to `torch.qint8` and\n",
    "   layers to a single-element set with `nn.Linear`.\n",
    "3. Save the model to a new variable (e.g. `model_quantized`), and print it to verify that linear layers have been\n",
    "   quantized properly (i.e. `DynamicQuantizedLinear` instead of `Linear`).\n",
    "4. Save both models to disk (`state_dict` for both) and compare the file sizes (e.g. `os.path.getsize()`).\n",
    "5. Compare the inference speed and speedup on CPU for original and quantized models (again, average of 100 runs).\n",
    "6. Display the comparison. Do you think that quantization is helpful in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc5d2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.ao.quantization import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54fd7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model\n",
      "MPNetModel(\n",
      "  (embeddings): MPNetEmbeddings(\n",
      "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): MPNetEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x MPNetLayer(\n",
      "        (attention): MPNetAttention(\n",
      "          (attn): MPNetSelfAttention(\n",
      "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (intermediate): MPNetIntermediate(\n",
      "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): MPNetOutput(\n",
      "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (relative_attention_bias): Embedding(32, 12)\n",
      "  )\n",
      "  (pooler): MPNetPooler(\n",
      "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "File size\n",
      "fp32 model size: 417.73 MB\n",
      "int8 model size: 173.10 MB\n",
      "size comparsion (int8 / fp32): 0.41\n"
     ]
    }
   ],
   "source": [
    "device_cpu = torch.device(\"cpu\")\n",
    "\n",
    "model_cpu = model.to(device_cpu)\n",
    "model_cpu.eval()\n",
    "\n",
    "inputs_cpu = {k: v.to(device_cpu) for k, v in inputs.items()}\n",
    "\n",
    "model_quantized = quantize_dynamic(\n",
    "    model_cpu,             \n",
    "    {nn.Linear},           \n",
    "    dtype=torch.qint8     \n",
    ")\n",
    "\n",
    "print(\"Quantized model\")\n",
    "print(model_quantized)\n",
    "\n",
    "\n",
    "fp32_path = \"model_fp32_state_dict.pth\"\n",
    "int8_path = \"model_quantized_int8_state_dict.pth\"\n",
    "\n",
    "torch.save(model_cpu.state_dict(), fp32_path)\n",
    "torch.save(model_quantized.state_dict(), int8_path)\n",
    "\n",
    "size_fp32 = os.path.getsize(fp32_path)\n",
    "size_int8 = os.path.getsize(int8_path)\n",
    "\n",
    "def to_MB(bytes_size):\n",
    "    return bytes_size / (1024 * 1024)\n",
    "\n",
    "print(\"File size\")\n",
    "print(f\"fp32 model size: {to_MB(size_fp32):.2f} MB\")\n",
    "print(f\"int8 model size: {to_MB(size_int8):.2f} MB\")\n",
    "print(f\"size comparsion (int8 / fp32): {size_int8 / size_fp32:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa76b3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference comparsion fp32 vs int8 on CPU\n",
      "fp32 model: 0.038781 s\n",
      "int8 model: 0.013071 s\n",
      "Speedup int8 vs fp32: 2.97x\n"
     ]
    }
   ],
   "source": [
    "def forward_fp32():\n",
    "    with torch.inference_mode():\n",
    "        _ = model_cpu(**inputs_cpu)\n",
    "\n",
    "\n",
    "def forward_int8():\n",
    "    with torch.inference_mode():\n",
    "        _ = model_quantized(**inputs_cpu)\n",
    "\n",
    "\n",
    "time_fp32 = measure_time(forward_fp32, n_runs=100)\n",
    "time_int8 = measure_time(forward_int8, n_runs=100)\n",
    "\n",
    "print(\"Inference comparsion fp32 vs int8 on CPU\")\n",
    "print(f\"fp32 model: {time_fp32:.6f} s\")\n",
    "print(f\"int8 model: {time_int8:.6f} s\")\n",
    "\n",
    "speedup_int8 = time_fp32 / time_int8\n",
    "print(f\"Speedup int8 vs fp32: {speedup_int8:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0a873",
   "metadata": {},
   "source": [
    "Quantization is helpful here. The int8 quantized model is faster and smaller than the model that uses fp32 precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_7MLProject (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
